{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bdf4925",
   "metadata": {},
   "outputs": [],
   "source": [
    "Обучить полносвязную модель на MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d80683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import argparse\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe121449",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST('.', download=True, train=True)\n",
    "test_dataset = datasets.MNIST('.', download=True, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0f026b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7da4281",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        nn.init.xavier_normal_(self.linear1.weight)         \n",
    "        self.activation1 = nn.Sigmoid()\n",
    "        self.do1 = nn.Dropout(dropout_p)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.activation2 = nn.Sigmoid()\n",
    "        self.do2 = nn.Dropout(dropout_p)\n",
    "        self.linear3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):                                  \n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.do1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.do2(x)\n",
    "        x = self.linear3(x)     \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b97793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data: list):\n",
    "    pics = []\n",
    "    target = []\n",
    "    for item in data:\n",
    "        pics.append(numpy.array(item[0]))\n",
    "        target.append(item[1])\n",
    "    pics = torch.from_numpy(numpy.array(pics)).float() / 255 \n",
    "    target = torch.from_numpy(numpy.array(target))\n",
    "\n",
    "    return {\n",
    "        'data': pics.view(pics.size(0), -1), \n",
    "        'target': target.long(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b096e455",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28 * 28\n",
    "hidden_dim = 128\n",
    "output_dim = 10\n",
    "device = torch.device('cuda')\n",
    "n_epochs = 15\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8b12c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(input_dim , hidden_dim, output_dim).to(device)\n",
    "optim = torch.optim.Adam(model.parameters()) \n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5354a799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss_train: 2.3620717525482178\n",
      "epoch: 0, step: 200, loss_train: 0.6787263751029968\n",
      "epoch: 0, step: 400, loss_train: 0.33667346835136414\n",
      "epoch: 0, loss_test: 0.3199388086795807\n",
      "epoch: 1, step: 0, loss_train: 0.28920456767082214\n",
      "epoch: 1, step: 200, loss_train: 0.2860707938671112\n",
      "epoch: 1, step: 400, loss_train: 0.27441006898880005\n",
      "epoch: 1, loss_test: 0.22577236592769623\n",
      "epoch: 2, step: 0, loss_train: 0.21219022572040558\n",
      "epoch: 2, step: 200, loss_train: 0.1912621110677719\n",
      "epoch: 2, step: 400, loss_train: 0.13444916903972626\n",
      "epoch: 2, loss_test: 0.1782744824886322\n",
      "epoch: 3, step: 0, loss_train: 0.19931180775165558\n",
      "epoch: 3, step: 200, loss_train: 0.09741687774658203\n",
      "epoch: 3, step: 400, loss_train: 0.07288753986358643\n",
      "epoch: 3, loss_test: 0.15003250539302826\n",
      "epoch: 4, step: 0, loss_train: 0.2588886022567749\n",
      "epoch: 4, step: 200, loss_train: 0.18691866099834442\n",
      "epoch: 4, step: 400, loss_train: 0.0895182341337204\n",
      "epoch: 4, loss_test: 0.1321171075105667\n",
      "epoch: 5, step: 0, loss_train: 0.14029499888420105\n",
      "epoch: 5, step: 200, loss_train: 0.22562453150749207\n",
      "epoch: 5, step: 400, loss_train: 0.13013717532157898\n",
      "epoch: 5, loss_test: 0.11629296839237213\n",
      "epoch: 6, step: 0, loss_train: 0.2005297690629959\n",
      "epoch: 6, step: 200, loss_train: 0.23132722079753876\n",
      "epoch: 6, step: 400, loss_train: 0.07999881356954575\n",
      "epoch: 6, loss_test: 0.10595008730888367\n",
      "epoch: 7, step: 0, loss_train: 0.1439412236213684\n",
      "epoch: 7, step: 200, loss_train: 0.07781046628952026\n",
      "epoch: 7, step: 400, loss_train: 0.09769657999277115\n",
      "epoch: 7, loss_test: 0.09795337170362473\n",
      "epoch: 8, step: 0, loss_train: 0.054713137447834015\n",
      "epoch: 8, step: 200, loss_train: 0.1288241446018219\n",
      "epoch: 8, step: 400, loss_train: 0.10204890370368958\n",
      "epoch: 8, loss_test: 0.09267017990350723\n",
      "epoch: 9, step: 0, loss_train: 0.16988269984722137\n",
      "epoch: 9, step: 200, loss_train: 0.13552583754062653\n",
      "epoch: 9, step: 400, loss_train: 0.07438697665929794\n",
      "epoch: 9, loss_test: 0.08580441027879715\n",
      "epoch: 10, step: 0, loss_train: 0.04844185337424278\n",
      "epoch: 10, step: 200, loss_train: 0.06466855853796005\n",
      "epoch: 10, step: 400, loss_train: 0.040785402059555054\n",
      "epoch: 10, loss_test: 0.08306590467691422\n",
      "epoch: 11, step: 0, loss_train: 0.05329161509871483\n",
      "epoch: 11, step: 200, loss_train: 0.0324154756963253\n",
      "epoch: 11, step: 400, loss_train: 0.06747746467590332\n",
      "epoch: 11, loss_test: 0.08165627717971802\n",
      "epoch: 12, step: 0, loss_train: 0.03942028060555458\n",
      "epoch: 12, step: 200, loss_train: 0.04881808161735535\n",
      "epoch: 12, step: 400, loss_train: 0.025501269847154617\n",
      "epoch: 12, loss_test: 0.07708151638507843\n",
      "epoch: 13, step: 0, loss_train: 0.05131933465600014\n",
      "epoch: 13, step: 200, loss_train: 0.04339921101927757\n",
      "epoch: 13, step: 400, loss_train: 0.1038951575756073\n",
      "epoch: 13, loss_test: 0.07590694725513458\n",
      "epoch: 14, step: 0, loss_train: 0.058817166835069656\n",
      "epoch: 14, step: 200, loss_train: 0.032147765159606934\n",
      "epoch: 14, step: 400, loss_train: 0.07447312772274017\n",
      "epoch: 14, loss_test: 0.07722077518701553\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    trainloader = DataLoader(train_dataset, \n",
    "                            batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, \n",
    "                             collate_fn=collate_fn, \n",
    "                             drop_last = True)\n",
    "    \n",
    "    model.train()\n",
    "    for i, batch in enumerate(trainloader):\n",
    "        optim.zero_grad()\n",
    "        predict = model(batch['data'].to(device))\n",
    "        loss = loss_func(predict, batch['target'].to(device))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if i % 200 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss_train: {loss.item()}')\n",
    "        \n",
    "    testloader = DataLoader(test_dataset, \n",
    "                            batch_size=len(test_dataset.data),\n",
    "                            shuffle=True, \n",
    "                            collate_fn=collate_fn, \n",
    "                            drop_last = False)\n",
    "        \n",
    "    model.eval()\n",
    "    for i, batch in enumerate(testloader):     \n",
    "        predict = model(batch['data'].to(device))\n",
    "        loss = loss_func(predict, batch['target'].to(device))\n",
    "        print(f'epoch: {epoch}, loss_test: {loss.item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae727be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Обучить глубокую сверточную сеть на MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f35f8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvModel(nn.Module):\n",
    "  def __init__(self, input_ch, hidden_ch, output_dim, dropout_p=0.2):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(input_ch, hidden_ch, kernel_size=5, padding=2, stride=2)\n",
    "    self.bn1 = nn.BatchNorm2d(hidden_ch)\n",
    "    self.activation1 = nn.Sigmoid()\n",
    "    self.do1 = nn.Dropout(dropout_p)\n",
    "    self.conv2 = nn.Conv2d(hidden_ch, hidden_ch, kernel_size=3, padding=1, stride=1)\n",
    "    self.bn2 = nn.BatchNorm2d(hidden_ch)\n",
    "    self.activation2 = nn.Sigmoid()\n",
    "    self.do2 = nn.Dropout(dropout_p)\n",
    "    self.conv3 = nn.Conv2d(hidden_ch, 15, kernel_size=3, padding=1, stride=1)       \n",
    "    self.classifier = nn.Linear(15 * 14 *14, output_dim) \n",
    "    \n",
    "    \n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.activation1(x)\n",
    "    x = self.do1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.bn2(x)\n",
    "    x = self.activation2(x)\n",
    "    x = self.do2(x)\n",
    "    x = self.conv3(x)\n",
    "    x = self.classifier(x.view(x.size(0), -1))\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dbb118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_cv(data: list):\n",
    "\n",
    "  pics = []\n",
    "  target = []\n",
    "  for item in data:\n",
    "    pics.append(numpy.array(item[0]))\n",
    "    target.append(item[1])\n",
    "  pics = torch.from_numpy(numpy.array(pics)).float() / 255 \n",
    "  target = torch.from_numpy(numpy.array(target))\n",
    "\n",
    "  return {\n",
    "      'data': pics.unsqueeze(1), \n",
    "      'target': target.long(),\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e79e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ch = 1\n",
    "hidden_ch = 128\n",
    "out_dim = 10\n",
    "device = torch.device('cuda')\n",
    "n_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2a6a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cv = ConvModel(input_ch, hidden_ch, out_dim).to(device)\n",
    "optim = torch.optim.Adam(model_cv.parameters())\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f83f5b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, train_loss: 2.327293872833252\n",
      "epoch: 0, step: 200, train_loss: 0.3287416696548462\n",
      "epoch: 0, step: 400, train_loss: 0.1748921424150467\n",
      "epoch: 0, test_loss: 0.18799753487110138\n",
      "epoch: 1, step: 0, train_loss: 0.33881354331970215\n",
      "epoch: 1, step: 200, train_loss: 0.13034258782863617\n",
      "epoch: 1, step: 400, train_loss: 0.1458151936531067\n",
      "epoch: 1, test_loss: 0.09573335200548172\n",
      "epoch: 2, step: 0, train_loss: 0.10515721887350082\n",
      "epoch: 2, step: 200, train_loss: 0.050386495888233185\n",
      "epoch: 2, step: 400, train_loss: 0.13618828356266022\n",
      "epoch: 2, test_loss: 0.06599275767803192\n",
      "epoch: 3, step: 0, train_loss: 0.021876882761716843\n",
      "epoch: 3, step: 200, train_loss: 0.09318149834871292\n",
      "epoch: 3, step: 400, train_loss: 0.1667996346950531\n",
      "epoch: 3, test_loss: 0.060153521597385406\n",
      "epoch: 4, step: 0, train_loss: 0.11107579618692398\n",
      "epoch: 4, step: 200, train_loss: 0.12923818826675415\n",
      "epoch: 4, step: 400, train_loss: 0.09675615280866623\n",
      "epoch: 4, test_loss: 0.057879138737916946\n",
      "epoch: 5, step: 0, train_loss: 0.04373670741915703\n",
      "epoch: 5, step: 200, train_loss: 0.0879635438323021\n",
      "epoch: 5, step: 400, train_loss: 0.12263253331184387\n",
      "epoch: 5, test_loss: 0.04616366699337959\n",
      "epoch: 6, step: 0, train_loss: 0.021580660715699196\n",
      "epoch: 6, step: 200, train_loss: 0.07326994091272354\n",
      "epoch: 6, step: 400, train_loss: 0.041599612683057785\n",
      "epoch: 6, test_loss: 0.047053273767232895\n",
      "epoch: 7, step: 0, train_loss: 0.08764223754405975\n",
      "epoch: 7, step: 200, train_loss: 0.02361663617193699\n",
      "epoch: 7, step: 400, train_loss: 0.046116217970848083\n",
      "epoch: 7, test_loss: 0.05407293885946274\n",
      "epoch: 8, step: 0, train_loss: 0.037125542759895325\n",
      "epoch: 8, step: 200, train_loss: 0.013670428656041622\n",
      "epoch: 8, step: 400, train_loss: 0.11177532374858856\n",
      "epoch: 8, test_loss: 0.0465044341981411\n",
      "epoch: 9, step: 0, train_loss: 0.11883664131164551\n",
      "epoch: 9, step: 200, train_loss: 0.05122905969619751\n",
      "epoch: 9, step: 400, train_loss: 0.05609346553683281\n",
      "epoch: 9, test_loss: 0.048831138759851456\n",
      "epoch: 10, step: 0, train_loss: 0.05272377282381058\n",
      "epoch: 10, step: 200, train_loss: 0.16384053230285645\n",
      "epoch: 10, step: 400, train_loss: 0.050901684910058975\n",
      "epoch: 10, test_loss: 0.04944593831896782\n",
      "epoch: 11, step: 0, train_loss: 0.07502966374158859\n",
      "epoch: 11, step: 200, train_loss: 0.12407906353473663\n",
      "epoch: 11, step: 400, train_loss: 0.0488910973072052\n",
      "epoch: 11, test_loss: 0.04297976195812225\n",
      "epoch: 12, step: 0, train_loss: 0.13189968466758728\n",
      "epoch: 12, step: 200, train_loss: 0.052281469106674194\n",
      "epoch: 12, step: 400, train_loss: 0.014978102408349514\n",
      "epoch: 12, test_loss: 0.040817610919475555\n",
      "epoch: 13, step: 0, train_loss: 0.08034766465425491\n",
      "epoch: 13, step: 200, train_loss: 0.04440474510192871\n",
      "epoch: 13, step: 400, train_loss: 0.12151606380939484\n",
      "epoch: 13, test_loss: 0.05016342177987099\n",
      "epoch: 14, step: 0, train_loss: 0.07181672006845474\n",
      "epoch: 14, step: 200, train_loss: 0.0601942203938961\n",
      "epoch: 14, step: 400, train_loss: 0.08155994862318039\n",
      "epoch: 14, test_loss: 0.04203997924923897\n",
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(n_epochs):\n",
    "        \n",
    "    trainloader = DataLoader(train_dataset, \n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, \n",
    "                             collate_fn=collate_fn_cv, \n",
    "                             drop_last = True)\n",
    "       \n",
    "    model_cv.train()  \n",
    "    for i, batch in enumerate(trainloader):      \n",
    "        optim.zero_grad()\n",
    "        predict = model_cv(batch['data'].to(device))\n",
    "        loss = loss_func(predict, batch['target'].to(device))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if i % 200 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, train_loss: {loss.item()}')\n",
    "    \n",
    "    testloader = DataLoader(test_dataset, \n",
    "                            batch_size=len(test_dataset.data),\n",
    "                            shuffle=True, \n",
    "                            collate_fn=collate_fn_cv, \n",
    "                            drop_last = False)\n",
    "    \n",
    "    model_cv.eval()\n",
    "    for i, batch in enumerate(testloader): \n",
    "        predict = model_cv(batch['data'].to(device))\n",
    "        loss = loss_func(predict, batch['target'].to(device))\n",
    "        print(f'epoch: {epoch}, test_loss: {loss.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
